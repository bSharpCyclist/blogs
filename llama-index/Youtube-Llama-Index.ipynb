{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarize YouTube Videos with LlamaIndex - Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "A few months ago I published an article showcasing how one could use [LlamaIndex](https://www.llamaindex.ai/) to summarize YouTube videos based on transcripts. LlamaIndex has changed a lot since then, and many of the links in that previous article are now dead, along with non functional code. Things change so quickly today, very hard to keep up! Nevertheless, I thought it would be fun to create a new project which uses the updated API. Note, at the time of this writing I'm using llama-index version 0.8.0.\n",
    "\n",
    "The areas that will be covered in this short article are the following:\n",
    "\n",
    "* YouTube transcript downloader\n",
    "* Loading transcripts and creating a VectorStore index\n",
    "* Persist index to disk and reload\n",
    "* Query index and examine nodes/chunks used by LLM\n",
    "* Using similarity_top_k to change the number of nodes/chunks sent to LLM\n",
    "* Using Llama-Index debugging to examine events\n",
    "* Refreshing an index with new documents\n",
    "* Using Jupyter-to-Medium to publish notebook to Medium\n",
    "\n",
    "Llama-Index has many different index structures that can be used, but this article will only be using the VectorStore index. Future articles will take a deeper dive into the different index structures and how to use them. Additionally, I also want to take a look at using local resources for embedding and response generation, as opposed to using OpenAI. \n",
    "\n",
    "## YouTube Transcript Downloader\n",
    "\n",
    "Before moving into the details of LlamaIndex, I'll post the code that I use to download transcripts from YouTube playlists. Once again, I'll use the Ancient Aliens playlist for my corpus of data to play with. UAPs and extraterrestrial biologics are quite an interesting topic these days. But if that doesn't suit you, then feel free to change the playlist ID and follow along with whatever topic/playlist that interests you!\n",
    "\n",
    "As mentioned before in the previous article, you will need to obtain a YouTube Data API key. The one major change I made to the script since last time was to only download new transcripts in the playlist that aren't already present in my local directory. I do this by sorting the videos by publish date, descending, and once I encounter a transcript I already have I then stop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install google-api-python-client\n",
    "# pip install youtube_transcript_api\n",
    "import googleapiclient.discovery\n",
    "import os\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "\n",
    "def save_transcripts_to_files(api_key, playlist_id, output_dir):\n",
    "    # Build the YouTube API client using the provided API key\n",
    "    youtube = googleapiclient.discovery.build(\"youtube\", \"v3\", developerKey=api_key)\n",
    "\n",
    "    # Get all the videos in the playlist, sorted by date\n",
    "    videos = []\n",
    "    next_page_token = None\n",
    "    while True:\n",
    "        request = youtube.playlistItems().list(\n",
    "            part=\"contentDetails,snippet\",\n",
    "            playlistId=playlist_id,\n",
    "            maxResults=50,\n",
    "            pageToken=next_page_token\n",
    "        )\n",
    "        response = request.execute()\n",
    "\n",
    "        # Add each video to the list of videos\n",
    "        for item in response[\"items\"]:\n",
    "            video_id = item[\"contentDetails\"][\"videoId\"]\n",
    "            video_title = item[\"snippet\"][\"title\"]\n",
    "            video_date = item[\"snippet\"][\"publishedAt\"]\n",
    "            videos.append((video_id, video_title, video_date))\n",
    "\n",
    "        # Check if there are more videos to fetch\n",
    "        next_page_token = response.get(\"nextPageToken\")\n",
    "        if not next_page_token:\n",
    "            break\n",
    "\n",
    "    # Sort the videos by date, descending. Once we reach a file that already exists, we can stop\n",
    "    # This allows us to run the script again later and only fetch new videos\n",
    "    videos.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "    # Create the output directory if it doesn't exist\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # For each video, get the transcript and save it to a file if it doesn't already exist\n",
    "    for video_id, video_title, video_date in videos:\n",
    "        try:\n",
    "            # Remove any non-alphanumeric characters from the video title and use it as the filename\n",
    "            safe_title = \"\".join([c for c in video_title if c.isalnum() or c.isspace()]).rstrip()\n",
    "            filename = os.path.join(output_dir, f\"{safe_title}.txt\")\n",
    "            if os.path.exists(filename):\n",
    "                # If the file already exists, assume the rest are there too and stop\n",
    "                break\n",
    "            transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "            with open(filename, \"w\") as file:\n",
    "                # Write each transcript entry to the file\n",
    "                for entry in transcript:\n",
    "                    file.write(entry['text'] + ' ')\n",
    "            print(f\"Transcript saved to {safe_title}.txt\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching transcript for video ID {video_id} ({video_title}): {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv('YOUTUBE_API_KEY')\n",
    "playlist_id = \"PLob1mZcVWOaiVxrCiEyYXcAbmx7UY8ggW\"\n",
    "output_dir = \"transcripts/ancient-aliens-official\"\n",
    "\n",
    "save_transcripts_to_files(api_key, playlist_id, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LlamaIndex\n",
    "\n",
    "The LlamaIndex [documentation](https://gpt-index.readthedocs.io/en/latest/index.html) has also changed quite a bit since I last used the project back in March. I highly recommend reading through the information a bit and getting a feel for what it can do. A good place to start would be the Basic Usage Pattern tutorial. It's pretty easy to get up and running quickly. You can read in the transcripts or whatever text you have, create an index, and then execute a query in just a few lines of code.\n",
    "\n",
    "Another great resource for learning LlamaIndex is their [Discord](https://discord.com/invite/eN6D2HQ4aX) server. It's very active and there are many people there that can help you out. Additionally, they have a chat bot that will answer your questions about how to use the API. It's amazing how well it works. I've learned so much about the API by just playing with some code and asking the chatbot questions. A lot faster than reading through the documentation, but there will be times that you'll want to go through it in more detail. The chatbot does a great job of pointing you in the right direction, and you can go from there.\n",
    "\n",
    "### Read Data and Create Index\n",
    "\n",
    "Let's go through the code below. The process begins by reading in the transcript text files using SimpleDirectoryReader. While doing so the filename gets added as metadata with a key of 'episode_title'. This is to help bias the search later based on episode title. Additionally, a flag is passed to indicate the filename should be used as the ID for the Document. This will come in handy later when refreshing the index based on new transcripts that are downloaded. Finally, each document in the collection is tagged such that the meta data isn't passed to the LLM, as it doesn't necessarily need to read that when generating a response. Again, the metadata is being used to help find the chunks of data to pass to the LLM.\n",
    "\n",
    "Next an LLM object is created using the gpt-4 model. Note, the default text generation model, if not specified, is gpt-3.5-turbo, and for creating the embeddings it is text-embedding-ada-002. When creating the LLM object, max tokens is set to 1024. This indicates the maximum length of the response we should get from the LLM. For gpt-4, the maximum number of tokens is 8192, and this includes both the input and output. More on that later.\n",
    "\n",
    "A ServiceContext object is then created, which is used during the indexing and querying stage in a LlamaIndex pipeline/application. Chunk_size is set to 1024 tokens, which indicates the size of the chunks that the transcript text data should be broken up into. And then finally, a VectorStoreIndex is created and saved to disk. Llama-Index has many types of indexes that can be used and will be dictated by the use case. Again, I hope to explore them more in future articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install llama-index\n",
    "# pip install ipywidgets\n",
    "# pip install nltk - needed for version 0.7.24 and greater\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import sys\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)  # Change INFO to DEBUG if you want more extensive logging\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
    "from llama_index.llms import OpenAI\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "transcript_directory = \"transcripts/ancient-aliens-official\"\n",
    "storage_directory = \"storage/ancient-aliens-official/vector\"\n",
    "\n",
    "# Add filename as metadata to each chunk associated with a document/transcript\n",
    "filename_fn = lambda filename: {'episode_title': filename}  #Future - consider using the below instead of full path\n",
    "#filename_fn = lambda filename: {'episode_title': os.path.splitext(os.path.basename(filename))[0]}\n",
    "documents = SimpleDirectoryReader(transcript_directory, filename_as_id=True, \n",
    "                                  file_metadata=filename_fn).load_data()\n",
    "\n",
    "# Exclude metadata from the LLM, meaning it won't read it when generating a response.\n",
    "# Future - consider looping over documents and setting the id_ to basename, instead of fullpath\n",
    "[document.excluded_llm_metadata_keys.append('episode_title') for document in documents]\n",
    "\n",
    "# max tokens will impact the length of the output from the LLM, for OpenAI the default is 256 tokens\n",
    "llm = OpenAI(temperature=0, max_tokens=1024, model=\"gpt-4\")\n",
    "\n",
    "# chunk_size - It defines the size of the chunks (or nodes) that documents are broken into when they are indexed by LlamaIndex\n",
    "service_context = ServiceContext.from_defaults(llm=llm, chunk_size=1024)\n",
    "\n",
    "# Build the index\n",
    "index = VectorStoreIndex.from_documents(documents, service_context=service_context, show_progress=True)\n",
    "\n",
    "# Persist the index to disk\n",
    "index.storage_context.persist(persist_dir=storage_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reload Index\n",
    "The index can be reloaded later so that one doesn't have to keep rebuilding an index. The code below can be used to accomplish that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now you can load the index from disk when needed, and not rebuild it each time.\n",
    "import logging\n",
    "import sys\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)  # Change INFO to DEBUG if you want more extensive logging\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
    "from llama_index import StorageContext, load_index_from_storage\n",
    "from llama_index.llms import OpenAI\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "transcript_directory = \"transcripts/ancient-aliens-official\"\n",
    "storage_directory = \"storage/ancient-aliens-official/vector\"\n",
    "\n",
    "llm = OpenAI(temperature=0, max_tokens=1024, model=\"gpt-4\")\n",
    "service_context = ServiceContext.from_defaults(llm=llm, chunk_size=1024)\n",
    "storage_context = StorageContext.from_defaults(persist_dir=storage_directory)\n",
    "index = load_index_from_storage(storage_context, service_context=service_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Index\n",
    "\n",
    "Now the fun part, let's query the index and see what sort of response we get back. The question is similar to what I asked in the previous article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"Summarize the Alien DNA Found in Octopus Genetics episode in 3 paragraphs\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine Source Nodes\n",
    "\n",
    "The above summary looks pretty good to me! So what nodes or chunks were used to provide this response? The following code below can be used to see that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the number of source nodes\n",
    "num_source_nodes = len(response.source_nodes)\n",
    "print(f\"Number of source nodes: {num_source_nodes}\")\n",
    "\n",
    "# Loop over source nodes and print meta data\n",
    "for s in response.source_nodes:\n",
    "    print(f\"Node Score: {s.score}\")\n",
    "    print(s.node.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we see that two nodes/chunks were used, the node for the actual transcript we are interested in, and another one. It turns out the the transcript of interest can fit into one node, it's relatively short. When making a vector store query, by default the top 2 node/chunks are retrieved from the index using cosine similarity between the embedding vector of the query and the embedding vectors in the index. The parameter that controls this is called similarity_top_k.\n",
    "\n",
    "Let's try another query by focusing on the Crystal Skulls full episode. This transcript has about 5500 words. The ratio between tokens and words is roughly 100 tokens to 75 words. Using that ratio, it can be calculated that there are about 7300 tokens in the transcript. Each chunk is 1024 tokens, which means a little over 7 nodes/chunks are needed to represent the data. Additionally, when creating a new chunk some of the previous chunk's information is included. This is called chunk overlap and the default is 20. So with all that being said, a similarity_top_k of 8 should be sufficient to capture all the chunks needed to send to the LLM.\n",
    "\n",
    "Let's check if the document was indeed broken up into 8 chunks. We can use the code below to get the nodes associated with a document and count them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.node_parser import SimpleNodeParser\n",
    "\n",
    "# Retrieve the document by id\n",
    "document_id = 'transcripts\\\\ancient-aliens-official\\\\Ancient Aliens The Crystal Skulls S6 E2  Full Episode.txt'\n",
    "document = [doc for doc in documents if doc.id_ == document_id] # this will return just 1 document\n",
    "\n",
    "# Parse the document into nodes\n",
    "parser = SimpleNodeParser()\n",
    "nodes = parser.get_nodes_from_documents([document[0]])\n",
    "\n",
    "# Count the number of nodes\n",
    "num_nodes = len(nodes)\n",
    "print('Number of nodes: ', num_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity Top K\n",
    "\n",
    "So our back of the envelope calculation does indeed work! Let's now try a query and use a value of 8 for similarity_top_k and then examine the source nodes that contributed to the response from the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(similarity_top_k=8)\n",
    "response = query_engine.query(\"Please provide a comprehensive summary of the Crystal Skulls full Episode in 3 detailed paragraphs.\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the number of source nodes\n",
    "num_source_nodes = len(response.source_nodes)\n",
    "print(f\"Number of source nodes: {num_source_nodes}\")\n",
    "\n",
    "# Loop over source nodes and print meta data\n",
    "for s in response.source_nodes:\n",
    "    print(f\"Node Score: {s.score}\")\n",
    "    print(s.node.node_id)\n",
    "    print(s.node.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you might be thinking here, how can we send 8 chunks of size 1024 tokens and return a response of 1024 tokens, when the maximum allowed number of tokens is 8192? LlamaIndex handles this situation by breaking up the matching results into chunks that will fit into the prompt. This concept is called “refining” answers in LlamaIndex. After LlamaIndex gets an initial answer from the first API call, it sends the next chunk(s) to the API, along with the previous answer, and asks the model to refine that answer.\n",
    "\n",
    "### Llama Debug Handler\n",
    "\n",
    "We can use the LlamaIndex debugging handler to explore this in more detail and take a look at the events that are happening. Note, according to the documentation, the below is a beta feature and so the API is subject to change.\n",
    "\n",
    "The below code sets up the debug handlers. If you want to debug what happens during index construction, you can uncomment the appropriate line of code below. But I didn't want to rebuild the index here, so I pass the updated ServiceContext to the query engine instead.\n",
    "\n",
    "Based on the analysis above, how many LLM calls should we see? I think 2! Given the size of the 8 chunks and the size of the output response we would like, we'll need 2 LLM calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.callbacks import CallbackManager, LlamaDebugHandler, CBEventType\n",
    "\n",
    "llama_debug = LlamaDebugHandler(print_trace_on_end=True)\n",
    "callback_manager = CallbackManager([llama_debug])\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    callback_manager=callback_manager, llm=llm\n",
    ")\n",
    "\n",
    "# If you want to debug what happens when constructing the index, you can use the following code\n",
    "#index = VectorStoreIndex.from_documents(documents, service_context=service_context, show_progress=True)\n",
    "\n",
    "query_engine = index.as_query_engine(similarity_top_k=8, service_context=service_context)\n",
    "response = query_engine.query(\"Please provide a comprehensive summary of the Crystal Skulls full Episode in 3 detailed paragraphs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print info on the LLM calls during the list index query\n",
    "print(llama_debug.get_event_time_info(CBEventType.LLM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we do indeed see 2 LLM calls! Now let's examine the inputs and outputs for each LLM call. Event pairs can be used to look at this in more detail. The input for the first LLM call will be represented by [0][0], and the output by [0][1]. And the input for the second LLM call will be represented by [1][0], and its output by [1][1].\n",
    "\n",
    "Below we create the event pairs and then print out the output of each LLM call. Since the output includes the payload information that was sent to begin with, I didn't bother printing out the input. Here we can see things like what model was used, token usage, etc. Notice the statements below (all other content information that was sent to the LLM was deleted to save space)\n",
    "\n",
    ">You are an expert Q&A system that is trusted around the world. Always answer the question using the provided context information, and not prior knowledge. Some rules to follow: Never directly reference the given context in your answer. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.\n",
    "\n",
    "The above is called the text_qa_prompt and is used in the first query. The answer to that and the next chunk or nodes are used in subsequent queries with a refine_template prompt. Both of these can be customized at query time like so,\n",
    "\n",
    "``` \n",
    "    query_engine = index.as_query_engine(\n",
    "        text_qa_template=<custom_qa_prompt>,\n",
    "        refine_template=<custom_refine_prompt>\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_pairs = llama_debug.get_llm_inputs_outputs()\n",
    "print(event_pairs[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(event_pairs[1][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refresh Index With New Documents\n",
    "\n",
    "Refreshing an index with documents is pretty easy. After adding a new transcript to the local collection, the code below can be used to update the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload documents\n",
    "filename_fn = lambda filename: {'episode_title': filename}\n",
    "documents = SimpleDirectoryReader(transcript_directory, filename_as_id=True, file_metadata=filename_fn).load_data()\n",
    "\n",
    "# Refresh the index\n",
    "refreshed_docs = index.refresh_ref_docs(documents,\n",
    "                                        update_kwargs={\"delete_kwargs\": {'delete_from_docstore': True}})\n",
    "\n",
    "# refreshed_docs is a list of True/False values indicating whether the document was refreshed\n",
    "# Print the number of refreshed documents by print the number of True values\n",
    "print(sum(refreshed_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected only a single document to be refreshed, so output the id_ or filename of the refreshed document.\n",
    "print(documents[refreshed_docs.index(True)].id_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally below another query is made. Notice though that the most recent documented inserted did not come back with the highest similarity score! Interesting :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(service_context=service_context)\n",
    "response = query_engine.query(\"What baffling unsolved mysteries surround Stonehenge?\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the number of source nodes\n",
    "num_source_nodes = len(response.source_nodes)\n",
    "print(f\"Number of source nodes: {num_source_nodes}\")\n",
    "\n",
    "# Loop over source nodes and print meta data\n",
    "for s in response.source_nodes:\n",
    "    print(f\"Node Score: {s.score}\")\n",
    "    print(s.node.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "### Jupyter-to-Medium\n",
    "This article was originally written as a Jupyter notebook in VSCode. I then used the [jupyter-to-medium](https://pypi.org/project/jupyter-to-medium/) library to push the notebook directly to Medium. There is some setup involved, e.g., obtaining a Medium integration token (see link). But once you have that, you can than push the notebook to Medium directly from the command line in a terminal. Note, the below command line syntax didn't work for me.\n",
    "\n",
    "jupyter_to_medium --pub-name=\"Dunder Data\" --tags=\"python, data science\" \"My Awesome Blog Post.ipynb\"\n",
    "\n",
    "All you really have to do is the below and should work just fine. From there you can go to Medium and tweak the article as you see fit. I did so with some of the code output to make it look nicer.\n",
    "\n",
    "jupyter_to_medium \"My Awesome Blog Post.ipynb\"\n",
    "\n",
    "Hope you enjoyed this article and/or found it useful! Feel free to ask any questions you might have. Thanks!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
